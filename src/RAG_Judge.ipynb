{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48dae128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/nlp/Exam/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fitz\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0dc22f",
   "metadata": {},
   "source": [
    "### Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09417ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing the card data\n",
    "df = pd.read_csv(\"../data/cards.csv\")\n",
    "df = df[~df[\"type\"].isin([\"Token\", \"Skill Card\"])]\n",
    "\n",
    "df_meta = df.drop(columns=['id','image_url', 'image_url_small', 'ban_tcg', 'ban_ocg', 'ban_goat',\n",
    "                            'staple', 'views', 'viewsweek', 'upvotes', 'downvotes', 'formats', 'treated_as', \n",
    "                            'tcg_date', 'ocg_date', 'konami_id', 'has_effect'])\n",
    "#df_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4aea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading test data\n",
    "test_df = pd.read_csv(\"../data/test_dataset.csv\")\n",
    "#test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ebf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This Chunk was only run once to convert the PDF file into text ###\n",
    "# Extracting text from rulebook pdf \n",
    "#def pdf_to_text(pdf_path):\n",
    "#    doc = fitz.open(pdf_path)\n",
    "#    text = \"\"\n",
    "#    for page in doc:\n",
    "#        text += page.get_text()\n",
    "#    return text\n",
    "\n",
    "#rules = pdf_to_text(\"../data/SD_RuleBook_EN_10.pdf\")\n",
    "\n",
    "# Saving the text as a .txt file for cleaning\n",
    "#with open('../data/Rules.txt', 'a', encoding='utf-8') as file:\n",
    "#    file.writelines(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7604de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the manually cleaned .txt file\n",
    "with open('../data/Manually_Cleaned_Rules.txt', 'r') as file:\n",
    "   clean_rules = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ea434",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sections(text):\n",
    "    \"\"\"\n",
    "    Splits a structured text document into (header, body) section pairs.\n",
    "\n",
    "    The function uses a regular expression to capture each header and then pairs it\n",
    "    with the text that follows until the next header. The result is a list\n",
    "    of (header, body) tuples in the order they appear in the document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The full text to be segmented into sections.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuple\n",
    "        A list where each element is a (header, body) pair:\n",
    "        - header : str\n",
    "            The extracted section header, e.g. \"3. Experimental Setup\".\n",
    "        - body : str\n",
    "            The text belonging to that section, stripped of surrounding\n",
    "            whitespace.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\n{3}(\\d{1,2}\\.\\s[^\\n]+)\\n{2}\"\n",
    "    parts = re.split(pattern, text)\n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i].strip()\n",
    "        body = parts[i+1].strip()\n",
    "        sections.append((header, body))\n",
    "    return sections\n",
    "\n",
    "\n",
    "def chunk_text(text, max_words=300, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits a long text into overlapping word-based chunks.\n",
    "\n",
    "    This function divides the input text into sequential chunks, each\n",
    "    containing up to `max_words` words. Consecutive chunks overlap by\n",
    "    `overlap` words to preserve context across boundaries. The final chunk\n",
    "    may contain fewer than `max_words` words if the text ends before the\n",
    "    limit is reached.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The full text to be segmented.\n",
    "    max_words : int, optional\n",
    "        Maximum number of words allowed in each chunk. Default is 300.\n",
    "    overlap : int, optional\n",
    "        Number of words shared between consecutive chunks. Default is 50.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A list of text chunks, each containing up to `max_words` words,\n",
    "        with `overlap` words of context preserved between adjacent chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + max_words, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += max_words - overlap  # move forward with overlap\n",
    "    return chunks\n",
    "\n",
    "def build_rag_chunks(cleaned_text):\n",
    "    \"\"\"\n",
    "    Constructs chunks by utilizing the previous 2 functions.\n",
    "\n",
    "    This function first splits the input text into sections using\n",
    "    `split_sections`, then further divides each section body into\n",
    "    overlapping word-based chunks via `chunk_text`. Each resulting chunk is\n",
    "    stored in a dictionary containing the section header, a unique\n",
    "    chunk identifier, and the chunk text itself. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cleaned_text : str\n",
    "        The full preprocessed text to be segmented into sections and\n",
    "        sub-chunks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        A list of chunk dictionaries with the following keys:\n",
    "        - \"section\": str\n",
    "            The section header from which the chunk was derived.\n",
    "        - \"chunk_id\": str\n",
    "            A unique identifier combining the section header and chunk index.\n",
    "        - \"text\": str\n",
    "            The chunked text content.\n",
    "    \"\"\"    \n",
    "    sections = split_sections(cleaned_text)\n",
    "    rag_chunks = []\n",
    "    for header, body in sections:\n",
    "        subchunks = chunk_text(body, max_words=300, overlap=50)\n",
    "        for i, chunk in enumerate(subchunks):\n",
    "            rag_chunks.append({\n",
    "                \"section\": header,\n",
    "                \"chunk_id\": f\"{header}_{i}\",\n",
    "                \"text\": chunk\n",
    "            })\n",
    "    return rag_chunks\n",
    "\n",
    "# Building chunks\n",
    "rag_chunks = build_rag_chunks(clean_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2e06e",
   "metadata": {},
   "source": [
    "### Creating Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aaeb5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "texts = [chunk[\"text\"] for chunk in rag_chunks]\n",
    "embeddings = embed_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "metadata = [\n",
    "    {\"section\": chunk[\"section\"], \"chunk_id\": chunk[\"chunk_id\"]}\n",
    "    for chunk in rag_chunks\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483cb8d",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71212737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 3 files: 100%|██████████| 3/3 [00:12<00:00,  4.00s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ruling(text):\n",
    "    \"\"\"\n",
    "    Parses the model-generated ruling into a structured decision and explanation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The raw ruling text produced by the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, str]\n",
    "        A pair consisting of:\n",
    "        - decision : {\"YES\", \"NO\", \"UNKNOWN\"}\n",
    "            The extracted decision label.\n",
    "        - explanation : str\n",
    "            The explanation text following the last \"Explanation:\" marker.\n",
    "    \"\"\"\n",
    "    # Normalize whitespace\n",
    "    cleaned = \" \".join(text.split())\n",
    "\n",
    "    # Extracts decision (YES/NO)\n",
    "    decision_match = re.search(r\"decision\\s*[:\\-]\\s*(yes|no)\", cleaned, re.IGNORECASE)\n",
    "    decision = decision_match.group(1).upper() if decision_match else \"UNKNOWN\"\n",
    "\n",
    "    # Find all occurrences of \"Explanation:\"\n",
    "    explanation_positions = [m.start() for m in re.finditer(r\"explanation\\s*[:\\-]\", cleaned, re.IGNORECASE)]\n",
    "\n",
    "    if not explanation_positions:\n",
    "        return decision, \"\"\n",
    "\n",
    "    # Use only the last Explanation:\n",
    "    last_pos = explanation_positions[-1]\n",
    "\n",
    "    # Extract everything after the last Explanation:\n",
    "    explanation = cleaned[last_pos:]\n",
    "    explanation = re.sub(r\"explanation\\s*[:\\-]\\s*\", \"\", explanation, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    return decision, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_text(card_names, df_meta):\n",
    "    \"\"\"\n",
    "    Retrieves formatted card descriptions from the metadata dataframe.\n",
    "\n",
    "    For each card name in `card_names`, this function looks up the\n",
    "    corresponding row in `df_meta` and extracts the card's description,\n",
    "    type, and sub-type (race). \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    card_names : list of str\n",
    "        A list of card names to retrieve metadata for.\n",
    "    df_meta : pandas.DataFrame\n",
    "        A dataframe containing card metadata, expected to include the\n",
    "        columns \"name\", \"desc\", \"type\", and \"race\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A formatted multi‑section string where each section contains:\n",
    "        - the card name\n",
    "        - its card type\n",
    "        - its sub-type (race)\n",
    "        - its descriptive text\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for name in card_names:\n",
    "        row = df_meta[df_meta[\"name\"] == name]\n",
    "        if not row.empty:\n",
    "            desc = row.iloc[0][\"desc\"]\n",
    "            ctype = row.iloc[0][\"type\"]\n",
    "            race = row.iloc[0][\"race\"]\n",
    "            texts.append(\n",
    "                f\"{name}:\\n\"\n",
    "                f\"  Card Type: {ctype}\\n\"\n",
    "                f\"  Sub-Type: {race}\\n\"\n",
    "                f\"  Text: {desc}\"\n",
    "            )\n",
    "    return \"\\n\\n\".join(texts)\n",
    "\n",
    "def build_retrieval_query(query, game_state, card_text):\n",
    "    return f\"Query: {query}\\nGame State: {game_state}\\nCard Text: {card_text}\"\n",
    "\n",
    "def retrieve(query, game_state, card_text, embed_model, index, metadata, rag_chunks, k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant RAG chunks for a given query and game state.\n",
    "\n",
    "    This function constructs a retrieval query using the user query, the\n",
    "    current game state, and any relevant card text. The combined query is\n",
    "    embedded and searched against a vector index. The top-k nearest chunks \n",
    "    are returned along with their associated metadata.\n",
    "    \"\"\"\n",
    "    retrieval_query = build_retrieval_query(query, game_state, card_text)\n",
    "    q_emb = embed_model.encode([retrieval_query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(q_emb, k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append({\n",
    "            \"section\": metadata[idx][\"section\"],\n",
    "            \"chunk_id\": metadata[idx][\"chunk_id\"],\n",
    "            \"text\": rag_chunks[idx][\"text\"]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44409ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(game_state, query, card_text, retrieved_chunks):\n",
    "    context = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[Section: {c['section']}]\\n{c['text']}\"\n",
    "        for c in retrieved_chunks\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are acting as a Yu-Gi-Oh! Judge.\n",
    "Answer the ruling question based only on the game state, card text, context and your internal knowledge.\n",
    "Give a YES or NO answer and a short explanation.\n",
    "\n",
    "### Game State\n",
    "{game_state}\n",
    "\n",
    "### Query\n",
    "{query}\n",
    "\n",
    "### Card Text\n",
    "{card_text}\n",
    "\n",
    "### Rulebook Context\n",
    "{context}\n",
    "\n",
    "### Ruling Format\n",
    "Decision: <YES/NO>\n",
    "Explanation: <short explanation>\n",
    "\"\"\"\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afabcda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(row, model, tokenizer, df_meta, index, metadata, rag_chunks, embed_model):\n",
    "    \"\"\"\n",
    "    Queries the language model for a Yu-Gi-Oh! ruling and returns a\n",
    "    structured interpretation of its output.\n",
    "    \"\"\"\n",
    "    cards = row[\"cards\"]\n",
    "\n",
    "    # Convert string → list\n",
    "    if isinstance(cards, str):\n",
    "        try:\n",
    "            cards = ast.literal_eval(cards)\n",
    "        except:\n",
    "            cards = [cards]\n",
    "\n",
    "    # Ensure list\n",
    "    if not isinstance(cards, list):\n",
    "        cards = [cards]\n",
    "\n",
    "    card_text = get_card_text(cards, df_meta)\n",
    "    #card_text = get_card_text(row[\"cards\"], df_meta)\n",
    "\n",
    "    retrieved = retrieve(\n",
    "        row[\"query\"],\n",
    "        row[\"game_state\"],\n",
    "        card_text,\n",
    "        embed_model,\n",
    "        index,\n",
    "        metadata,\n",
    "        rag_chunks,\n",
    "        k=5 # Change this for number of chunks retrieved\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Build RAG prompt\n",
    "    prompt = build_rag_prompt(\n",
    "        row[\"game_state\"],\n",
    "        row[\"query\"],\n",
    "        card_text,\n",
    "        retrieved\n",
    "    )\n",
    "\n",
    "    # Run Mistral\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Parse ruling\n",
    "    decision, explanation = parse_ruling(text)\n",
    "\n",
    "    return {\n",
    "        \"model_decision\": decision,\n",
    "        \"model_explanation\": explanation,\n",
    "        \"raw_output\": text,\n",
    "        \"retrieved_sections\": [c[\"section\"] for c in retrieved]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcd8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate a Ruling & a Explanation for each scenario\n",
    "for idx, row in test_df.iterrows():\n",
    "    result = ask_rag(\n",
    "        row,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        df_meta,\n",
    "        index,\n",
    "        metadata,\n",
    "        rag_chunks,\n",
    "        embed_model\n",
    "    )\n",
    "\n",
    "    test_df.at[idx, \"rag_decision\"] = result[\"model_decision\"]\n",
    "    test_df.at[idx, \"rag_explanation\"] = result[\"model_explanation\"]\n",
    "    test_df.at[idx, \"rag_raw\"] = result[\"raw_output\"]\n",
    "    test_df.at[idx, \"rag_sections\"] = str(result[\"retrieved_sections\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcd9dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame one level up, inside results/\n",
    "test_df.to_csv(\"../results/yugioh_rulings_rag_5chunks_text.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
